# @package _global_

defaults:
  - override /model: bert_base_uncased.yaml
  - override /datamodule: mnli.yaml

name: weak/undertrain/bert/mnli
seed: 888

model: 
  # following the details on GitHub 
  # (https://github.com/UKPLab/emnlp2020-debiasing-unknown/blob/main/src/train_distill_bert.py)
  # TODO: I dont know which is better, I didn't get good results with this turned on
  # weight_decay: 0.01
  # warmup_steps: 0.1
  # learning_rate: 5e-5

datamodule:
  batch_size: 32
  select_train_samples: 2000

trainer:
  max_epochs: 5 # Paper said 3, GitHub repo said 5!
  gpus: 1
