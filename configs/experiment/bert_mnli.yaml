# @package _global_

defaults:
  - override /mode: exp.yaml
  - override /trainer: default.yaml
  - override /model: null
  - override /datamodule: null
  - override /callbacks: custom.yaml
  - override /logger: wandb.yaml

name: bert_mnli
seed: 42
test_after_training: False # MNLI cannot be tested, test set labels are hidden

model:
  _target_: src.models.hf_model.SequenceClassificationTransformer
  num_labels: 3
  huggingface_model: bert-base-uncased
  learning_rate: 3e-5
  batch_size: ${datamodule.batch_size}
  loss_fn:
    _target_: torch.nn.CrossEntropyLoss
  use_teacher_probs: False

datamodule:
  _target_: src.datamodules.hf_datamodule.HFDataModule
  dataset_name: glue
  subdataset_name: mnli
  tokenizer_name: ${model.huggingface_model}
  sentence_1_name: premise
  sentence_2_name: hypothesis
  batch_size: 128
  num_workers: 8
  pin_memory: True

trainer:
  max_epochs: 3
  gpus: 1