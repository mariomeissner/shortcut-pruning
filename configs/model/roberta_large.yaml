_target_: src.models.hf_model.SequenceClassificationTransformer

num_labels: ${datamodule.num_labels}
huggingface_model: "roberta-large"
learning_rate: 2e-5
warmup_steps: 0.20 # RoBERTa paper said 6% ofer 10 epochs with early stopping (so 20)
weight_decay: 0.1
batch_size: ${datamodule.batch_size}
loss_fn: "crossentropy"
use_bias_probs: False
