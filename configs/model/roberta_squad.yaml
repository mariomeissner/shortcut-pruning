_target_: src.models.squad_model.QuestionAnsweringTransformer

huggingface_model: "roberta-large"
learning_rate: 2e-5
batch_size: ${datamodule.batch_size}
warmup_steps: 0.20 # RoBERTa paper said 6% over 10 epochs with early stopping (so 20)
weight_decay: 0.1
loss_fn: "crossentropy"
use_bias_probs: False
